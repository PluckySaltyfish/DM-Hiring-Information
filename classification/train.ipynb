{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对各招聘信息的企业人才需求画像进行了建模，该模型能通过求职者的基本信息（渴望最低工资、最高工资、学历、工作经验、工作地点），生成可求职企业的基本画像（公司性质与公司规模）。该功能可以帮助各招聘门户网站完善其求职搜索功能，在用户进行搜索后迅速缩小适合企业的范围，进行更加迅速有效的职位推荐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from math import cos, sin, atan2, sqrt, pi, radians, degrees, asin\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该问题的本质是多标签分类问题。Multi-Label Machine Learning(MLL算法)是指预测模型中存在多个y值，具体分为两类不同情况：  \n",
    "- 多个预测y值  \n",
    "- 在分类模型中，一个样例可能存在多个不固定的类别  \n",
    "\n",
    "在本项目中，通过输入用户信息，预测公司的两个标签，为`公司性质`与`公司规模`。  \n",
    "\n",
    "根据多标签问题的复杂性，可以将问题分为两大类：  \n",
    "- 待预测值之间存在相互的依赖关系\n",
    "- 待预测值之间不存在依赖关系  \n",
    "\n",
    "多标签问题的学习策略主要有三种，分别是一阶策略，二阶策略和高阶策略。  \n",
    "1. 一阶策略不考虑标签相关性，效率高；\n",
    "2. 二阶策略考虑两个标签之间的相关性；\n",
    "3. 高阶策略考虑多个标签之间的相关性，性能好。  \n",
    "\n",
    "目前有很多关于多标签的学习算法，依据解决问题的角度，这些算法可以分为两大类:一是`问题转换策略`，二是`算法适应性策略`。  \n",
    "`问题转换策略`是转化问题数据，使之使用现有算法,是一种将多标签的分类问题转换成为单标签模型构造的问题，然后将模型合并的一种方式。可分为：  \n",
    "- Binary Relevance(二元关联)：标签之间无关联  \n",
    "- Classifier Chains(分类器链)：标签之间有依赖关系  \n",
    "- Calibrated Label Ranking(LP法)：两两标签之间有关系  \n",
    "\n",
    "`算法适应性策略`是指针对某一特定的算法进行扩展，从而能够处理多标记数据，改进算法，适用数据。主要有：  \n",
    "- Multi Label-KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据文件，抽取出特征和标签，并对其进行独热编码，按`4:1`的比例进行训练集和测试集的划分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['c_name', 'c_nature', 'c_scale', 'w_place', 'w_field', 'w_experience',\n",
       "       'education', 's_min', 's_max', 'vacancies'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/info-final.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df_unsampled):\n",
    "    df_other = df_unsampled.drop(df[df['c_nature'] == '民营'].index, axis = 0)\n",
    "    df0 = df_unsampled.groupby(['c_nature','c_scale']).get_group(('民营','20人以下'))\n",
    "    df1 = df_unsampled.groupby(['c_nature','c_scale']).get_group(('民营','20-99人')).sample(n=500)\n",
    "    df2 = df_unsampled.groupby(['c_nature','c_scale']).get_group(('民营','100-499人')).sample(n=500)\n",
    "    df3 = df_unsampled.groupby(['c_nature','c_scale']).get_group(('民营','500-999人')).sample(n=500)\n",
    "    df4 = df_unsampled.groupby(['c_nature','c_scale']).get_group(('民营','1000-9999人')).sample(n=500)\n",
    "    df5 = df_unsampled.groupby(['c_nature','c_scale']).get_group(('民营','10000人以上'))\n",
    "    df_sampled = pd.concat([df_other, df0, df1, df2, df3, df4, df5], axis = 0)\n",
    "    return df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info = pd.DataFrame(df, columns = ['c_nature','c_scale'])\n",
    "features = df.drop(['c_name', 'c_nature', 'c_scale', 'w_field', 'vacancies'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = np.array(pd.get_dummies(features))\n",
    "company_info = np.array(pd.get_dummies(company_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder()\n",
    "# features.w_place = le.fit_transform(features.w_place)\n",
    "# features.w_experience = le.fit_transform(features.w_experience)\n",
    "# features.education = le.fit_transform(features.education)\n",
    "# features.s_min = le.fit_transform(features.s_min)\n",
    "# features.s_max = le.fit_transform(features.s_max)\n",
    "# company_info.c_nature = le.fit_transform(company_info.c_nature)\n",
    "# company_info.c_scale = le.fit_transform(company_info.c_scale)\n",
    "# ohe = OneHotEncoder()\n",
    "# features = ohe.fit_transform(features)\n",
    "# company_info = ohe.fit_transform(company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# features = mlb.fit_transform(features.values)\n",
    "# company_info = mlb.fit_transform(company_info.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, company_info, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Transformation Methods (转换策略)\n",
    "### Binary Relevance（二元关联）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Relevance的核心思想是将多标签分类问题进行分解，将其转换为q个二元分类问题，其中每个二元分类器对应一个待预测的标签。  \n",
    "由于没有考虑标签之间的相关性，是**一阶策略**。  \n",
    "优点： \n",
    "- 估计单标签分类器；\n",
    "- 可以推广到超出标签组合的范围；\n",
    "- 实现方式简单，容易理解；\n",
    "- 当y值之间不存在相关的依赖关系的时候，模型的效果不错  \n",
    "\n",
    "缺点：  \n",
    "- 标签数目很多的时候不适合；\n",
    "- 忽略标签之间的相关性；\n",
    "- 如果y直接存在相互的依赖关系，那么最终构建的模型的泛化能力比较弱；\n",
    "- 需要构建q个二分类器，q为待预测的y值数量，当q比较大的时候，需要构建的模型会比较多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造二分类器的方法使用one-vs-rest的方式。可以直接使用如下接口实现，其中的基分类器可以使用任意sklearn中的预设分类器。  \n",
    "使用scikit-multilearn工具包进行策略及模型的构建与训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Decision Tree决策树分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7305110033569564"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "classifier = BinaryRelevance(DecisionTreeClassifier())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Naive Bayes朴素贝叶斯模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16797928008879962"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用SVM分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8103486077614558"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "classifier = BinaryRelevance(SVC())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Random Forest Classifier随即森林分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7370261317629738"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "classifier = BinaryRelevance(RandomForestClassifier())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Chains (分类器链)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Chains的核心思想是将多标签分类问题进行分解，将其转换成为一个二元分类器链的形式，其中链后的二元分类器的构建式在前面分类器预测结果的基础上的。在模型构建的时候，首先将标签顺序进行shuffle打乱排序操作，然后按照从头到尾分别构建每个标签对应的模型。  \n",
    "虽然还是作为二分类问题解决的，但以链式的方式随机考虑了多个标签的相关性，这是**高阶策略**。  \n",
    "优点：  \n",
    "- 实现方式相对比较简单，容易理解；  \n",
    "- 考虑标签之间的依赖关系，最终模型的泛化能力相对于Binary Relevance方式构建的模型效果要好。  \n",
    "\n",
    "缺点：  \n",
    "- 很难找到一个比较适合的标签依赖关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5928023153391216"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "classifier = ClassifierChain(DecisionTreeClassifier())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4583333333333333"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "classifier = ClassifierChain(GaussianNB())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.601963746223565"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "classifier = ClassifierChain(SVC())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071248741188319"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "classifier = ClassifierChain(RandomForestClassifier())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Powerset (LP法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Powerset的核心思想是将多标签学习转化为一个多类分类问题，每个不同的标签组合认为是一个不同的类。  \n",
    "考虑了多个标签之间的相关性，是**高阶策略**。  \n",
    "\n",
    "优点：\n",
    "- 利用一个分类器考虑了多标签的相关性；\n",
    "- 在训练数据包括全部标签组合时通常是最优解。  \n",
    "\n",
    "缺点\n",
    "- 要求训练数据包括所有的标签组合；\n",
    "- 当标签空间大时，很容易过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6125377643504532"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "classifier = LabelPowerset(DecisionTreeClassifier())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3588872104733132"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "classifier = LabelPowerset(GaussianNB())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62865055387714"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "classifier = LabelPowerset(SVC())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6098942598187311"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "classifier = LabelPowerset(RandomForestClassifier())\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Adaptation (算法适应性策略)\n",
    "### MLKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLKNN的核心思想是对于每一个实例而言，先获取距离它最近的K个实例，然后使用这些实例的标签集合，通过最大后验概率(MAP)来判断这个实例的预测标签集合的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/bigdata/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass n_neighbors=10 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7525525525525526"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLKNN\n",
    "classifier = MLkNN(k=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "parameters = {'k': range(1,10), 's': [0.5, 0.7, 1.0]}\n",
    "score = 'accuracy'\n",
    "\n",
    "clf = GridSearchCV(MLkNN(), parameters, scoring=score)\n",
    "clf.fit(X_train, y_train)\n",
    "print (clf.best_params_, clf.best_score_)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于传统的基于统计的方法最终准确率结果不佳，我们采用MLP神经网络模型进行训练和预测。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Activation\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15886 samples, validate on 3972 samples\n",
      "Epoch 1/10\n",
      "15886/15886 - 1s - loss: 0.3776 - accuracy: 0.8953 - val_loss: 0.2298 - val_accuracy: 0.9248\n",
      "Epoch 2/10\n",
      "15886/15886 - 1s - loss: 0.2107 - accuracy: 0.9252 - val_loss: 0.2000 - val_accuracy: 0.9252\n",
      "Epoch 3/10\n",
      "15886/15886 - 1s - loss: 0.1962 - accuracy: 0.9262 - val_loss: 0.1924 - val_accuracy: 0.9266\n",
      "Epoch 4/10\n",
      "15886/15886 - 1s - loss: 0.1906 - accuracy: 0.9278 - val_loss: 0.1881 - val_accuracy: 0.9277\n",
      "Epoch 5/10\n",
      "15886/15886 - 1s - loss: 0.1871 - accuracy: 0.9290 - val_loss: 0.1851 - val_accuracy: 0.9290\n",
      "Epoch 6/10\n",
      "15886/15886 - 1s - loss: 0.1844 - accuracy: 0.9295 - val_loss: 0.1827 - val_accuracy: 0.9293\n",
      "Epoch 7/10\n",
      "15886/15886 - 1s - loss: 0.1823 - accuracy: 0.9299 - val_loss: 0.1807 - val_accuracy: 0.9299\n",
      "Epoch 8/10\n",
      "15886/15886 - 1s - loss: 0.1805 - accuracy: 0.9303 - val_loss: 0.1790 - val_accuracy: 0.9302\n",
      "Epoch 9/10\n",
      "15886/15886 - 1s - loss: 0.1789 - accuracy: 0.9304 - val_loss: 0.1776 - val_accuracy: 0.9306\n",
      "Epoch 10/10\n",
      "15886/15886 - 1s - loss: 0.1776 - accuracy: 0.9306 - val_loss: 0.1763 - val_accuracy: 0.9304\n",
      "3972/3972 [==============================] - 0s 20us/sample - loss: 0.1763 - accuracy: 0.9304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17632813691672722, 0.9304381]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = X_train.shape[0]\n",
    "dim_no = X_train.shape[1]\n",
    "class_no = y_train.shape[1]\n",
    "# create simple mlp\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(class_no, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "# train\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2, validation_data=(X_test,y_test))\n",
    "# evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了获得统计上可信的结果，我们将实验重复20次并报告平均结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  模型   | 准确率  |  \n",
    "|  :----  | ---- |\n",
    "| **Binary Relevance** |\n",
    "| DT | 0.731 |\n",
    "| NB | 0.296 |\n",
    "| SVM | 0.810 |\n",
    "| RFC | 0.737 |\n",
    "| **Classifier Chains** |\n",
    "| DT | 0.593 |\n",
    "| NB | 0.458 |\n",
    "| SVM | 0.602 |\n",
    "| RFC | 0.607 |\n",
    "| **Label Powerset** |\n",
    "| DT | 0.612 |\n",
    "| NB | 0.358 |\n",
    "| SVM | 0.628 |\n",
    "| RFC | 0.610 |\n",
    "| **Algorithm Adaptation** |\n",
    "| MLKNN | 0.752 |\n",
    "| **Neural Network** |\n",
    "| MLP | 0.931 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由以上结果，经过我们的分析，首先由于朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好，所以无论何种转换策略，使用朴素贝叶斯模型分类的结果都不理想。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
